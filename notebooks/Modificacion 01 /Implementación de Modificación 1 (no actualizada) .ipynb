{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMamlEAbKOrO"
   },
   "source": [
    "# Implementaci칩n Modificaci칩n 1\n",
    "Differentiable Logic Cellular Automata: from Game of Life to Pattern Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q_ToewoMBfk"
   },
   "source": [
    "**Note for the reader (as of March 20, 2025):**: We tested reproducibility of the results reported in\n",
    "this article using publicly available Colab notebooks with T4 GPUs. Determinism\n",
    "is ensured by setting the os.environ['XLA_FLAGS'] = '\n",
    "--xla_gpu_deterministic_ops=true' flag, which resolves non-determinism caused by\n",
    "specific JAX optimization routines, though at the cost of speed and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxZse8uV-C3D"
   },
   "outputs": [],
   "source": [
    "# We tested reproducibility on jax version 0.4.33\n",
    "\n",
    "!pip install jax[cuda12_pip]==0.4.33\n",
    "!pip install jaxlib==0.4.33\n",
    "\n",
    "# You might need to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ii6ioJvju78Q"
   },
   "outputs": [],
   "source": [
    "\"\"\"Reproducibility:\n",
    "\n",
    "This flag is necessary for ensuring reproducibility on Colab.\n",
    "Removing it significantly boosts performance (5-10x) but compromises\n",
    "result consistency due to inherent GPU non-determinism.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['XLA_FLAGS'] = ' --xla_gpu_deterministic_ops=true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWYAAFqsDV_Q"
   },
   "outputs": [],
   "source": [
    "# @title Imports and Notebook Utilities\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import io\n",
    "import itertools\n",
    "from base64 import b64decode\n",
    "from typing import List, Optional, Sequence, Tuple\n",
    "\n",
    "from einops import rearrange\n",
    "import flax.linen as nn\n",
    "import flax.linen as nn\n",
    "from IPython.display import HTML, Image, clear_output\n",
    "import jax\n",
    "print(jax.__version__)\n",
    "from jax import grad\n",
    "from jax.lax import conv_general_dilated_patches\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from jax.tree_util import tree_leaves, tree_map\n",
    "from matplotlib import colors\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "import optax\n",
    "import PIL\n",
    "import PIL.Image, PIL.ImageDraw, PIL.ImageFont, PIL.ImageOps\n",
    "import requests\n",
    "import networkx as nx  #Nueva\n",
    "\n",
    "\n",
    "os.environ['FFMPEG_BINARY'] = 'ffmpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYKUs0uhu78Q"
   },
   "outputs": [],
   "source": [
    "# @title Hyperparameters\n",
    "\n",
    "# Index of the pass-through gate used for network initialization.\n",
    "PASS_THROUGH_GATE = 3\n",
    "DEFAULT_PASS_VALUE = 10.0\n",
    "\n",
    "# Number of possible gates with 2 inputs and 1 output.\n",
    "NUMBER_OF_GATES = 16\n",
    "\n",
    "\"\"\"\n",
    "Fire rate for asynchronicity.\n",
    "\n",
    "60% of cells will perform the update,\n",
    "40% will have the update masked.\n",
    "\"\"\"\n",
    "FIRE_RATE = 0.6\n",
    "\n",
    "TARGET_EMOJI = \"游붍\"\n",
    "TARGET_SIZE_EMOJI = 20\n",
    "TARGET_SIZE_G = 16\n",
    "TARGET_SIZE_ASYNC = 14\n",
    "TARGET_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_adjacency_tensor(G, N_total, max_k=9, c_idx=4):\n",
    "    \"\"\"\n",
    "    Convierte un grafo k=8-regular de NetworkX en un tensor de 칤ndices de JAX \n",
    "    normalizado a 9 inputs.\n",
    "\n",
    "    Args:\n",
    "        G: Objeto nx.Graph (debe ser k=8-regular).\n",
    "        N_total: N칰mero total de nodos.\n",
    "        max_k: Tama침o fijo del vecindario (9).\n",
    "        c_idx: 칈ndice donde se coloca el nodo central (4).\n",
    "\n",
    "    Returns:\n",
    "        JAX array de 칤ndices con shape (N_total, 9).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Usamos un 칤ndice de padding que apunta a la fila de ceros agregada a las features.\n",
    "    padding_index = N_total \n",
    "    \n",
    "    # Inicializar el tensor de 칤ndices fijos (N_total, 9) con el valor del nodo central\n",
    "    adj_indices_padded_np = np.full((N_total, max_k), padding_index, dtype=np.int32)\n",
    "    \n",
    "    for i in range(N_total):\n",
    "        # Obtener los 8 vecinos reales del nodo i\n",
    "        neighbors = list(G.neighbors(i))\n",
    "        \n",
    "        # --- 1. Colocar el nodo central ---\n",
    "        # La posici칩n 4 es el propio nodo i.\n",
    "        adj_indices_padded_np[i, c_idx] = i \n",
    "        \n",
    "        # --- 2. Colocar a los 8 vecinos en las 8 posiciones restantes ---\n",
    "        \n",
    "        # 칈ndices disponibles en el patch (excluyendo la posici칩n central 4)\n",
    "        available_indices = [idx for idx in range(max_k) if idx != c_idx]\n",
    "        \n",
    "        # El grafo k=8-regular garantiza 8 vecinos, por lo que llenamos las 8 posiciones.\n",
    "        for j, neighbor_idx in enumerate(neighbors):\n",
    "            if j < len(available_indices): # Siempre True si |neighbors| <= 8\n",
    "                # Asignar el vecino a la j-칠sima posici칩n disponible\n",
    "                adj_indices_padded_np[i, available_indices[j]] = neighbor_idx\n",
    "            # Nota: Si el grafo es estrictamente k=8-regular, no se requiere padding (else).\n",
    "                \n",
    "    # --- 3. Convertir a JAX Array ---\n",
    "    return jnp.array(adj_indices_padded_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1xwG6JvEeeI"
   },
   "outputs": [],
   "source": [
    "# @title Utils and Plotting\n",
    "\n",
    "\n",
    "def zoom(img, scale=4):\n",
    "  img = np.repeat(img, scale, 0)\n",
    "  img = np.repeat(img, scale, 1)\n",
    "  return img\n",
    "\n",
    "\n",
    "def load_byte(byte, max_size=TARGET_SIZE):\n",
    "  img = PIL.Image.open(io.BytesIO(byte))\n",
    "  img.thumbnail((max_size, max_size), PIL.Image.LANCZOS)\n",
    "  img = np.float32(img) / 255.0\n",
    "  # premultiply RGB by Alpha\n",
    "  img[..., :3] *= img[..., 3:]\n",
    "  return img\n",
    "\n",
    "def visualize(frames, namefile, size, negate=1):\n",
    "  # Create and save animation\n",
    "  fig, ax = plt.subplots(figsize=(size, size))\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "\n",
    "  def animate(frame):\n",
    "    ax.clear()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    if negate:\n",
    "      frame = 1 - frame\n",
    "\n",
    "    return [ax.imshow(frame, cmap='binary')]\n",
    "\n",
    "  anim = animation.FuncAnimation(fig, animate, frames=frames, interval=200, blit=True)\n",
    "\n",
    "  # Save the gif\n",
    "  writer = animation.PillowWriter(fps=2)\n",
    "  anim.save(namefile, writer=writer)\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "def plot_show(img, negate=1):\n",
    "  if negate:\n",
    "    img = 1 - img\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "  ax.imshow(img, cmap='binary')\n",
    "  ax.axis('off')\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# Plot the histogram related to the statistics of the gates.\n",
    "def plot_hist_gates(\n",
    "    net, save_path=None, title='Distribution of Gates (Sorted)'\n",
    "):\n",
    "  gate_types = [\n",
    "      'FALSE',\n",
    "      'AND',\n",
    "      'A AND (NOT B)',\n",
    "      'A',\n",
    "      '(NOT A) AND B',\n",
    "      'B',\n",
    "      'XOR',\n",
    "      'OR',\n",
    "      'NOR',\n",
    "      'XNOR',\n",
    "      'NOT B',\n",
    "      'A OR (NOT B)',\n",
    "      'NOT A',\n",
    "      '(NOT A) OR B',\n",
    "      'NAND',\n",
    "      'TRUE',\n",
    "  ]\n",
    "\n",
    "  gate_counts = np.bincount(net, minlength=len(gate_types))\n",
    "  sorted_indices = np.argsort(gate_counts)\n",
    "  sorted_counts = gate_counts[sorted_indices]\n",
    "  sorted_gate_types = [gate_types[i] for i in sorted_indices]\n",
    "  fig, ax = plt.subplots(figsize=(12, 6), dpi=100)\n",
    "\n",
    "  ax.barh(\n",
    "      sorted_gate_types,\n",
    "      sorted_counts,\n",
    "      color='#2E86C1',\n",
    "      alpha=0.7,\n",
    "      edgecolor='white',\n",
    "      linewidth=1,\n",
    "  )\n",
    "\n",
    "  ax.set_xlabel('#Gates', fontsize=12, fontweight='bold')\n",
    "  ax.set_ylabel('Type of gate', fontsize=12, fontweight='bold')\n",
    "  ax.set_title(\n",
    "      'Distribution of Gates (Sorted)', fontsize=14, fontweight='bold', pad=20\n",
    "  )\n",
    "\n",
    "  plt.style.use('seaborn-v0_8-darkgrid')\n",
    "  ax.grid(True, linestyle='--', alpha=0.7, axis='x')\n",
    "  ax.spines['top'].set_visible(False)\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "  ax.set_facecolor('#f8f9fa')\n",
    "  fig.set_facecolor('white')\n",
    "  plt.tight_layout()\n",
    "\n",
    "  if save_path:\n",
    "    with open(save_path, 'wb') as f:\n",
    "      plt.savefig(f, format='svg')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# Plot the training loss functions.\n",
    "def plot_training_progress(\n",
    "    loss_train, loss_test, compute_every, save_path=None\n",
    "):\n",
    "  plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "\n",
    "  ax.grid(True, color='gray', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "\n",
    "  ax.plot(\n",
    "      loss_train,\n",
    "      color='#2E86C1',\n",
    "      linewidth=2,\n",
    "      label='Soft Gates Loss',\n",
    "      alpha=0.9,\n",
    "  )\n",
    "\n",
    "  ax.plot(\n",
    "      np.arange(0, len(loss_train), compute_every),\n",
    "      loss_test,\n",
    "      color='#E74C3C',\n",
    "      linestyle='--',\n",
    "      linewidth=2,\n",
    "      label='Hard Gates Loss',\n",
    "      alpha=0.9,\n",
    "  )\n",
    "\n",
    "  ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "  ax.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "  ax.set_ylabel('Loss Value', fontsize=12, fontweight='bold')\n",
    "  ax.set_title(\n",
    "      'Training Progress - Soft vs Hard Gates',\n",
    "      fontsize=14,\n",
    "      fontweight='bold',\n",
    "      pad=20,\n",
    "  )\n",
    "\n",
    "  ax.legend(\n",
    "      frameon=True, fancybox=True, shadow=True, fontsize=10, loc='upper right'\n",
    "  )\n",
    "\n",
    "  for spine in ax.spines.values():\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "  ax.set_facecolor('#f8f9fa')\n",
    "  fig.set_facecolor('white')\n",
    "\n",
    "  plt.tight_layout()\n",
    "\n",
    "  if save_path:\n",
    "    with open(save_path, 'wb') as f:\n",
    "      plt.savefig(f, format='svg')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Funci칩n de Python: prepare_adjacency_tensor para Grafo k=8\n",
    "\n",
    "\n",
    "def prepare_adjacency_tensor(G, N_total, max_k=9, c_idx=4):\n",
    "    \"\"\"\n",
    "    Convierte un grafo k=8-regular de NetworkX en un tensor de 칤ndices de JAX \n",
    "    normalizado a 9 inputs.\n",
    "\n",
    "    Args:\n",
    "        G: Objeto nx.Graph (debe ser k=8-regular).\n",
    "        N_total: N칰mero total de nodos.\n",
    "        max_k: Tama침o fijo del vecindario (9).\n",
    "        c_idx: 칈ndice donde se coloca el nodo central (4).\n",
    "\n",
    "    Returns:\n",
    "        JAX array de 칤ndices con shape (N_total, 9).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Usamos un 칤ndice de padding que apunta a la fila de ceros agregada a las features.\n",
    "    padding_index = N_total \n",
    "    \n",
    "    # Inicializar el tensor de 칤ndices fijos (N_total, 9) con el valor del nodo central\n",
    "    adj_indices_padded_np = np.full((N_total, max_k), padding_index, dtype=np.int32)\n",
    "    \n",
    "    for i in range(N_total):\n",
    "        # Obtener los 8 vecinos reales del nodo i\n",
    "        neighbors = list(G.neighbors(i))\n",
    "        \n",
    "        # --- 1. Colocar el nodo central ---\n",
    "        # La posici칩n 4 es el propio nodo i.\n",
    "        adj_indices_padded_np[i, c_idx] = i \n",
    "        \n",
    "        # --- 2. Colocar a los 8 vecinos en las 8 posiciones restantes ---\n",
    "        \n",
    "        # 칈ndices disponibles en el patch (excluyendo la posici칩n central 4)\n",
    "        available_indices = [idx for idx in range(max_k) if idx != c_idx]\n",
    "        \n",
    "        # El grafo k=8-regular garantiza 8 vecinos, por lo que llenamos las 8 posiciones.\n",
    "        for j, neighbor_idx in enumerate(neighbors):\n",
    "            if j < len(available_indices): # Siempre True si |neighbors| <= 8\n",
    "                # Asignar el vecino a la j-칠sima posici칩n disponible\n",
    "                adj_indices_padded_np[i, available_indices[j]] = neighbor_idx\n",
    "            # Nota: Si el grafo es estrictamente k=8-regular, no se requiere padding (else).\n",
    "                \n",
    "    # --- 3. Convertir a JAX Array ---\n",
    "    return jnp.array(adj_indices_padded_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNkdywCOEir4"
   },
   "outputs": [],
   "source": [
    "# @title Model definition\n",
    "\n",
    "\n",
    "def get_moore_connections(key):\n",
    "  \"\"\"Generate Moore neighborhood connections for a 9x1 vector.\n",
    "\n",
    "  Center element is at index 4 and connects to all other elements.\n",
    "  \"\"\"\n",
    "  neighbors = jnp.array([0, 1, 2, 3, 5, 6, 7, 8])\n",
    "  a = neighbors\n",
    "  b = jnp.full_like(neighbors, 4)\n",
    "  perm = jax.random.permutation(key, neighbors.shape[0])\n",
    "  a = a[perm]\n",
    "  b = b[perm]\n",
    "  return a, b\n",
    "\n",
    "\n",
    "# From https://github.com/Felix-Petersen/difflogic/tree/main/difflogic\n",
    "def get_unique_connections(in_dim, out_dim, key):\n",
    "  assert (\n",
    "      out_dim * 2 >= in_dim\n",
    "  )  # Number of neurons must not be smaller than half of inputs\n",
    "  x = jnp.arange(in_dim)\n",
    "  # Take pairs (0, 1), (2, 3), (4, 5), ...\n",
    "  a = x[::2]\n",
    "  b = x[1::2]\n",
    "  m = min(a.shape[0], b.shape[0])\n",
    "  a = a[:m]\n",
    "  b = b[:m]\n",
    "  # If needed, add pairs (1, 2), (3, 4), (5, 6), ...\n",
    "  if a.shape[0] < out_dim:\n",
    "    a_ = x[1::2]\n",
    "    b_ = x[2::2]\n",
    "    m = min(a_.shape[0], b_.shape[0])\n",
    "    a = jnp.concatenate([a, a_[:m]])\n",
    "    b = jnp.concatenate([b, b_[:m]])\n",
    "  # If still needed, add pairs with larger offsets\n",
    "  offset = 2\n",
    "  while out_dim > a.shape[0] and offset < in_dim:\n",
    "    a_ = x[:-offset]\n",
    "    b_ = x[offset:]\n",
    "    a = jnp.concatenate([a, a_])\n",
    "    b = jnp.concatenate([b, b_])\n",
    "    offset += 1\n",
    "\n",
    "  if a.shape[0] >= out_dim:\n",
    "    a = a[:out_dim]\n",
    "    b = b[:out_dim]\n",
    "  else:\n",
    "    raise ValueError(\n",
    "        f'Could not generate enough unique connections: {a.shape[0]} <'\n",
    "        f' {out_dim}'\n",
    "    )\n",
    "\n",
    "  # Random permutation\n",
    "  perm = jax.random.permutation(key, out_dim)\n",
    "  a = a[perm]\n",
    "  b = b[perm]\n",
    "\n",
    "  return a, b\n",
    "\n",
    "\n",
    "def bin_op_all_combinations(a, b):\n",
    "  # Implementation of binary operations between two inputs for all the different operations\n",
    "  return jnp.stack(\n",
    "      [\n",
    "          jnp.zeros_like(a),\n",
    "          a * b,\n",
    "          a - a * b,\n",
    "          a,\n",
    "          b - a * b,\n",
    "          b,\n",
    "          a + b - 2 * a * b,\n",
    "          a + b - a * b,\n",
    "          1 - (a + b - a * b),\n",
    "          1 - (a + b - 2 * a * b),\n",
    "          1 - b,\n",
    "          1 - b + a * b,\n",
    "          1 - a,\n",
    "          1 - a + a * b,\n",
    "          1 - a * b,\n",
    "          jnp.ones_like(a),\n",
    "      ],\n",
    "      axis=-1,\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "def bin_op_s(a, b, i_s):\n",
    "  # Compute all possible operations\n",
    "  combinations = bin_op_all_combinations(\n",
    "      a, b\n",
    "  )  # Shape: (n_gate, n_possible_gates, 16)\n",
    "  \"\"\"\n",
    "    Calculate the weighted sum of all possible gate operations.\n",
    "    During training (soft decoding), the weights are the probabilities\n",
    "    of each gate type. During inference (hard decoding), the weights\n",
    "    are a one-hot vector representing the selected gate type.\n",
    "    \"\"\"\n",
    "  result = jax.numpy.sum(combinations * i_s[None, ...], axis=-1)\n",
    "  return result\n",
    "\n",
    "\n",
    "def decode_soft(weights):\n",
    "  # From the weights vector compute the probability distribution of choosing each gate using softmax\n",
    "  return nn.softmax(weights, axis=-1)\n",
    "\n",
    "\n",
    "def decode_hard(weights):\n",
    "  return jax.nn.one_hot(\n",
    "      jnp.argmax(weights, axis=-1), 16\n",
    "  )  # Return the gate with maximum probability.\n",
    "\n",
    "\n",
    "# Initialize gates as pass through gate\n",
    "def init_gates(\n",
    "    n,\n",
    "    num_gates=NUMBER_OF_GATES,\n",
    "    pass_through_gate=PASS_THROUGH_GATE,\n",
    "    default_pass_value=DEFAULT_PASS_VALUE,\n",
    "):\n",
    "  \"\"\"Initializes a gate matrix with default pass-through values.\n",
    "\n",
    "  Args:\n",
    "      n: The number of rows in the gate matrix.\n",
    "      num_gates: The number of gates (columns). Defaults to NUMBER_OF_GATES.\n",
    "      pass_through_gate: The index of the pass-through gate column. Defaults to\n",
    "        PASS_THROUGH_GATE.\n",
    "      default_pass_value: The default value for the pass-through gate. Defaults\n",
    "        to DEFAULT_PASS_VALUE.\n",
    "\n",
    "  Returns:\n",
    "      An array representing the initialized gate matrix.\n",
    "  \"\"\"\n",
    "  gates = jnp.zeros((n, num_gates))\n",
    "  gates = gates.at[:, pass_through_gate].set(default_pass_value)\n",
    "  return gates\n",
    "\n",
    "\n",
    "def init_gate_layer(key, in_dim, out_dim, connections):\n",
    "\n",
    "  # With 'random' connections the input of each gate are sampled randomly from the previous layer.\n",
    "  if connections == 'random':\n",
    "    key1, key2 = jax.random.split(key)\n",
    "\n",
    "    c = jax.random.permutation(key2, 2 * out_dim) % in_dim\n",
    "    c = jax.random.permutation(key1, in_dim)[c]\n",
    "\n",
    "    c = c.reshape(2, out_dim)\n",
    "\n",
    "    indices_a = c[0, :]\n",
    "    indices_b = c[1, :]\n",
    "\n",
    "  # With 'unique' connections each gate will have a different pair of inputs.\n",
    "  elif connections == 'unique':\n",
    "    indices_a, indices_b = get_unique_connections(in_dim, out_dim, key)\n",
    "\n",
    "  # With 'first_kernel' the connections are specifically designed to mimic the Moore neighborhood.\n",
    "  \n",
    "  elif connections == 'first_kernel':                     #**\n",
    "    indices_a, indices_b = get_moore_connections(key)\n",
    "  else:\n",
    "    raise ValueError(f'Connection type {connections} not implemented')\n",
    "\n",
    "  wires = [indices_a, indices_b]\n",
    "  gate_logits = init_gates(out_dim)\n",
    "  return gate_logits, wires\n",
    "\n",
    "\n",
    "def init_logic_gate_network(hyperparams, params, wires, key):\n",
    "  for i, (in_dim, out_dim) in enumerate(\n",
    "      zip(hyperparams['layers'][:-1], hyperparams['layers'][1:])\n",
    "  ):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    gate_logits, gate_wires = init_gate_layer(\n",
    "        subkey, int(in_dim), int(out_dim), hyperparams['connections'][i]   #**\n",
    "    )\n",
    "    params.append(gate_logits)\n",
    "    wires.append(gate_wires)\n",
    "\n",
    "\n",
    "def init_perceive_network(hyperparams, params, wires, key):\n",
    "  for i, (in_dim, out_dim) in enumerate(\n",
    "      zip(hyperparams['layers'][:-1], hyperparams['layers'][1:])\n",
    "  ):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    gate_logits, gate_wires = init_gate_layer(\n",
    "        subkey, int(in_dim), int(out_dim), hyperparams['connections'][i]\n",
    "    )\n",
    "    \"\"\"\n",
    "    Replicate the gate logits for each of the 'n_kernels' perception kernels.\n",
    "    This allows for parallel computation of the perception module,\n",
    "    as all kernels share the same underlying structure and wiring.\n",
    "    \"\"\"\n",
    "    params.append(\n",
    "        gate_logits.repeat(hyperparams['n_kernels'], axis=0).reshape(\n",
    "            hyperparams['n_kernels'], out_dim, NUMBER_OF_GATES\n",
    "        )\n",
    "    )\n",
    "    wires.append(gate_wires)\n",
    "\n",
    "\n",
    "# In the current implementation all the kernels share the same connection scheme\n",
    "def init_diff_logic_ca(hyperparams, key):\n",
    "\n",
    "  key, subkey = jax.random.split(key)\n",
    "  \"\"\"Initialize parameters for the update and perceive networks.\n",
    "\n",
    "   Each network's parameters will be stored as a list of gate logits.\n",
    "\n",
    "   'update':  List of gate logits for the update network.\n",
    "              Each element is a JAX array with shape (out_dim, 16),\n",
    "              where out_dim is the output dimension of the layer.\n",
    "   'perceive': List of gate logits for the perceive network.\n",
    "               Each element is a JAX array with shape (n_kernels * out_dim, 16),\n",
    "               where n_kernels is the number of perception kernels\n",
    "               and out_dim is the output dimension of the layer.\n",
    "  \"\"\"\n",
    "  params = {'update': [], 'perceive': []}\n",
    "\n",
    "  \"\"\"Initialize wiring for the update and perceive networks.\n",
    "\n",
    "   Each network's wiring will be stored as a list of connection indices.\n",
    "\n",
    "   'update':  List of connection indices for the update network.\n",
    "              Each element is a tuple of two JAX arrays (indices_a, indices_b)\n",
    "              representing the input indices for each gate.\n",
    "   'perceive': List of connection indices for the perceive network.\n",
    "               The perceive kernels share the same wiring, so this list\n",
    "               contains a single tuple of connection indices (indices_a, indices_b).\n",
    "  \"\"\"\n",
    "  wires = {'update': [], 'perceive': []}\n",
    "\n",
    "  # Initialize the gate\n",
    "  init_logic_gate_network(\n",
    "      hyperparams['update'], params['update'], wires['update'], subkey    \n",
    "  )\n",
    "\n",
    "  key, subkey = jax.random.split(key)\n",
    "\n",
    "  # Initialize perceive vector\n",
    "  init_perceive_network(\n",
    "      hyperparams['perceive'], params['perceive'], wires['perceive'], subkey\n",
    "  )\n",
    "\n",
    "  return params, wires\n",
    "\n",
    "\n",
    "def run_layer(logits, wires, x, training):\n",
    "  \"\"\"Args:\n",
    "\n",
    "      x: input vector, shape (input_dim, 1).\n",
    "      wires: wire configuration, shape (out_dim, 1).\n",
    "      logits: gate parameters, shape (n_out, 16).\n",
    "\n",
    "  Returns:\n",
    "      If training is True, the expected gate values.\n",
    "      If training is False, the gates with maximum probability.\n",
    "  \"\"\"\n",
    "\n",
    "  a = x[..., wires[0]]\n",
    "  b = x[..., wires[1]]\n",
    "  logits = jax.lax.cond(training, decode_soft, decode_hard, logits)\n",
    "  out = bin_op_s(a, b, logits)\n",
    "  return out\n",
    "\n",
    "\n",
    "def run_update(params, wires, x, training):\n",
    "  for g, c in zip(params, wires):\n",
    "    x = run_layer(g, c, x, training)\n",
    "  return x\n",
    "\n",
    "\n",
    "def run_perceive(params, wires, x, training):\n",
    "  \"\"\"Applies a perception layer to a patch.\n",
    "\n",
    "  Args:\n",
    "      params: List of kernel parameters for each layer.\n",
    "      wires: List of wire configurations for each layer.\n",
    "      x: Input patch, shape [batch_size, patch_size, channel_size].\n",
    "      training: Boolean indicating training mode.\n",
    "\n",
    "  Returns:\n",
    "      Output feature vector, shape [batch_size, channel_size].\n",
    "  \"\"\"\n",
    "\n",
    "  # Apply each layer using vmap for kernel parallelism.\n",
    "  run_layer_map = jax.vmap(run_layer, in_axes=(0, None, 0, None))\n",
    "  x_prev = x\n",
    "  x = x.T  # [channel_size, batch_size, patch_size]\n",
    "\n",
    "  \"\"\"\n",
    "    Duplicate 'x' to create n_kernels copies for the first layer, \n",
    "    which all share the same input.\n",
    "    Subsequent layers receive unique inputs.\n",
    "    \"\"\"\n",
    "  x = jnp.repeat(\n",
    "      x[None, ...], params[0].shape[0], axis=0\n",
    "  )  # [n_kernels, channel_size, batch_size, patch_size]\n",
    "\n",
    "  # Iterate through layers, applying kernels and wire configurations.\n",
    "  for g, c in zip(params, wires):\n",
    "    x = run_layer_map(g, c, x, training)\n",
    "\n",
    "  x = rearrange(                              #**\n",
    "      x, 'k c s -> (c s k)'\n",
    "  )  # [channel_size * patch_size * n_kernels]\n",
    "\n",
    "  return jnp.concatenate(                          #**\n",
    "      [x_prev[4, :], x], axis=-1\n",
    "  )  # Concatenate the original input.\n",
    "\n",
    "\n",
    "def run_circuit(params, wires, x, training):\n",
    "  x = run_perceive(params['perceive'], wires['perceive'], x, training)\n",
    "  x = run_update(params['update'], wires['update'], x, training)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzCeEp-eu78Q"
   },
   "outputs": [],
   "source": [
    "# Test cases\n",
    "key = jax.random.PRNGKey(0)\n",
    "key_init, key = jax.random.split(key, 2)\n",
    "n_kernels = 4\n",
    "layers = [128] * 10 + [64, 32, 16]\n",
    "connections = ['unique'] * len(layers)\n",
    "\n",
    "\"\"\"Recap: Structure of the hyperparameters:\n",
    "\n",
    "'update' is referring to the diff logic network of the update.\n",
    "- Layers:  number of gates per layer (with exception of the first layer\n",
    "           which represent the number of inputs)\n",
    "- Connections: specify the topology of the connections, how the gates\n",
    "           are connected. It can be mainly 'random', 'unique' or 'first_kernel'\n",
    "           which will be used to the first kernel layer.\n",
    "\n",
    "'perceive' is referring to the diff logic networks of the perception\n",
    "          (or \"convolutional kernels\").\n",
    "- Layers:  number of gates per layer (with exception of the first layer which\n",
    "           represent the number of inputs)\n",
    "- Connections: specify the topology of the connections, how the gates are connected.\n",
    "           It can be mainly 'random', 'unique' or 'first_kernel' which is used\n",
    "           only for the first kernel layer to enforce connections that mimic the cell-interactions.\n",
    "\n",
    "For simplicity, all perception kernels share the same structure (type of connections,\n",
    "number of layers, and number of gates per layer). They are evaluated in parallel.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "hyperparams = {\n",
    "    'update': {'layers': layers, 'connections': connections},\n",
    "    'perceive': {\n",
    "        'n_kernels': n_kernels,\n",
    "        'layers': [9, 8, 4, 2, 1],      #Cambio: Modificar el primer valor en la lista 'layers' dentro de perceive de 9 a $N$. Para abrir la topologia\n",
    "        'connections': ['first_kernel', 'unique', 'unique', 'unique'],\n",
    "    },\n",
    "}\n",
    "\n",
    "training = True\n",
    "params, wires = init_diff_logic_ca(hyperparams, key_init)\n",
    "print(\n",
    "    params['perceive'][0]\n",
    "    .reshape((\n",
    "        hyperparams['perceive']['n_kernels'],\n",
    "        hyperparams['perceive']['layers'][1],\n",
    "        16,\n",
    "    ))\n",
    "    .shape\n",
    ")\n",
    "grid_ch_dim = 16\n",
    "inp = jnp.ones((3 * 3, grid_ch_dim)).astype(jnp.float32)\n",
    "x = run_circuit(params, wires, inp, training)\n",
    "print('INPUT SHAPE', inp.shape)\n",
    "print('OUTPUT SHAPE', x.shape)\n",
    "\n",
    "# The output of the circuit is the new state for the central cell with shape (1, grid_ch_dim)\n",
    "assert x.shape == (1, grid_ch_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Nueva Funci칩n: get_adjacency_patches\n",
    "\n",
    "# Se utiliza la variable PADDING_INDEX definida en el bloque anterior (Celda 1)\n",
    "# para el padding. Es un valor est치tico fuera de la funci칩n JIT.\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2, 3))\n",
    "def get_adjacency_patches(node_features, adjacency_indices_padded, max_neighbors=9, center_index=4):\n",
    "  \"\"\"\n",
    "  Realiza la agregaci칩n de features de los vecinos consultando el tensor de adyacencia\n",
    "  pre-procesado y normalizado.\n",
    "  \"\"\"\n",
    "  \n",
    "  # A침adir la fila de padding (ceros) al final del array de features para manejar el PADDING_INDEX\n",
    "  padding_feature = jnp.zeros_like(node_features[:1]) \n",
    "  node_features_padded = jnp.concatenate([node_features, padding_feature], axis=0)\n",
    "\n",
    "  # jnp.take selecciona los features de los nodos de entrada bas치ndose en los 칤ndices normalizados.\n",
    "  # El 칤ndice N_total_nodes (PADDING_INDEX) apuntar치 a la fila de ceros reci칠n a침adida.\n",
    "  patches = jnp.take(\n",
    "      node_features_padded,\n",
    "      adjacency_indices_padded,\n",
    "      axis=0\n",
    "  )\n",
    "  \n",
    "  return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7JD6xxhu78Q"
   },
   "outputs": [],
   "source": [
    "# @title Test (Modificado para usar l칩gica de Grafo)\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "N = 3 # Grilla 3x3 = 9 nodos\n",
    "C = 1\n",
    "patch_size = 3 \n",
    "N_total = N * N # 9 nodos\n",
    "\n",
    "# 1. Definir el tensor de adyacencia de PRUEBA (N=9, k=8)\n",
    "# Para un test simple, podemos simular que cada nodo es el centro y que los 9 inputs\n",
    "# son simplemente [0, 1, 2, 3, 4, 5, 6, 7, 8] (sin un grafo k=8 real)\n",
    "# En una prueba real, crear칤as un grafo k=8 de 9 nodos aqu칤.\n",
    "\n",
    "# Creamos un tensor que solo mapea las features en orden (simulando 9 inputs distintos)\n",
    "adjacency_indices_padded_test = jnp.arange(N_total * 9).reshape(N_total, 9) % N_total\n",
    "\n",
    "\n",
    "grid = random.randint(key, (N, N, C), minval=0, maxval=2).astype(jnp.float32)\n",
    "\n",
    "# Aplanar el input de la grilla a features de nodo (N_total_nodes, C)\n",
    "node_features = grid.reshape(-1, grid.shape[-1]) \n",
    "\n",
    "# 2. Reemplazar la funci칩n de cuadr칤cula con la funci칩n de grafo\n",
    "patches = get_adjacency_patches(\n",
    "    node_features, \n",
    "    adjacency_indices_padded_test, \n",
    "    max_neighbors=patch_size * patch_size # 9\n",
    ")\n",
    "\n",
    "print(f\"Grid\\n {grid[..., 0]}\")\n",
    "\n",
    "# NOTA: Las siguientes l칤neas de impresi칩n ya no tienen sentido porque patches ya no es un arreglo 2D convolucional\n",
    "# print(patches[-1, :, 1].reshape(1, 3, 3)) \n",
    "# print(patches[-1:, 1].reshape(-1))\n",
    "\n",
    "# La aserci칩n final sigue siendo v치lida si patches tiene el shape esperado (9, 9, 1)\n",
    "assert patches.shape == (N * N, patch_size * patch_size, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rblos5LlWJWn"
   },
   "outputs": [],
   "source": [
    "# @title run iterations\n",
    "\n",
    "\n",
    "# patches = [batch_size, n_patches, patch_size x patch_size, channels]\n",
    "def v_run_circuit_patched(patches, params, wires, training):\n",
    "  run_circuit_patch = jax.vmap(\n",
    "      run_circuit, in_axes=(None, None, 0, None)\n",
    "  )  # vmap over the patches\n",
    "  x = run_circuit_patch(params, wires, patches, training)\n",
    "  return x\n",
    "\n",
    "@jax.jit\n",
    "# Aceptar adjacency_indices_padded\n",
    "def run_async(grid, params, wires, training, periodic, key, adjacency_indices_padded):\n",
    "\n",
    "\n",
    "  node_features = grid.reshape(-1, grid.shape[-1]) # Aplanar la grilla a features de nodo (N_total_nodes, C)\n",
    "    \n",
    "  # REEMPLAZAR la l칩gica de cuadr칤cula con la l칩gica de grafo\n",
    "  # patches = get_grid_patches(...) # REMOVER\n",
    "  patches = get_adjacency_patches(node_features, adjacency_indices_padded) # <-- NUEVO\n",
    "    \n",
    "  x_new = v_run_circuit_patched(patches, params, wires, training)\n",
    "  x_new = x_new.reshape(*grid.shape)\n",
    "  #x_new = x_new.reshape(grid.shape) (primer run corrio con esto)\n",
    "  return x_new\n",
    "  # ... resto de la l칩gica async ...\n",
    "  update_mask_f32 = (\n",
    "      jax.random.uniform(key, x_new[..., :1].shape) <= FIRE_RATE\n",
    "  ).astype(jax.numpy.float32)\n",
    "  x = grid * (1 - update_mask_f32) + x_new * update_mask_f32\n",
    "  return x\n",
    "\n",
    "@jax.jit\n",
    "# Aceptar adjacency_indices_padded\n",
    "def run_sync(grid, params, wires, training, periodic, adjacency_indices_padded):\n",
    "\n",
    "  node_features = grid.reshape(-1, grid.shape[-1])   # Aplanar la grilla a features de nodo (N_total_nodes, C)\n",
    "    \n",
    "  # REEMPLAZAR la l칩gica de cuadr칤cula con la l칩gica de grafo\n",
    "  # patches = get_grid_patches(...) # REMOVER\n",
    "  patches = get_adjacency_patches(node_features, adjacency_indices_padded) # <-- NUEVO\n",
    "    \n",
    "  x_new = v_run_circuit_patched(patches, params, wires, training)\n",
    "  x_new = x_new.reshape(*grid.shape)\n",
    "  #x_new = x_new.reshape(grid.shape[0], grid.shape[1], grid.shape[2]) (primer run corrio con esto)\n",
    "  return x_new\n",
    "\n",
    "@partial(jax.jit, static_argnames=['num_steps', 'periodic', 'async_training'])\n",
    "# Aceptar adjacency_indices_padded\n",
    "def run_iter_nca(grid, params, wires, training, periodic, num_steps, async_training, key, adjacency_indices_padded):\n",
    "  def body_fn(carry, i):\n",
    "    grid, key = carry\n",
    "    if async_training:\n",
    "      key, subkey = jax.random.split(key)\n",
    "      # Pasar el nuevo argumento\n",
    "      x = run_async(grid, params, wires, training, periodic, subkey, adjacency_indices_padded) \n",
    "    else:\n",
    "      # Pasar el nuevo argumento\n",
    "      x = run_sync(grid, params, wires, training, periodic, adjacency_indices_padded) \n",
    "    return (x, key), 0\n",
    "\n",
    "  (grid, key), _ = jax.lax.scan(\n",
    "      body_fn, (grid, key), jnp.arange(0, num_steps, 1)\n",
    "  )\n",
    "  return grid\n",
    "\n",
    "\n",
    "v_run_iter_nca = jax.vmap(\n",
    "    run_iter_nca, in_axes=(0, None, None, None, None, None, None, None, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nueva @title Definiciones de Estado y P칠rdida (Requerido)\n",
    "\n",
    "TrainState = namedtuple('TrainState', 'param opt_state key')\n",
    "\n",
    "def init_state(hyperparams, opt, seed):\n",
    "  key = random.PRNGKey(seed)\n",
    "  key, subkey = random.split(key, 2)\n",
    "  params, wires = init_diff_logic_ca(hyperparams, subkey)\n",
    "  opt_state = opt.init(params)\n",
    "  return TrainState(params, opt_state, key), wires\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwPbY80TUujV"
   },
   "outputs": [],
   "source": [
    "# @title base64 lizard image\n",
    "'''\n",
    "Importing the Lizard image in base64 format.\n",
    "'''\n",
    "lizard_byte = b64decode(\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27oclTiTUvsX"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRSOcey-nskV"
   },
   "outputs": [],
   "source": [
    "# @title Learning to Grow the lizard\n",
    "\n",
    "# Outline of the lizard, creating a mask with 1 and 0 to use as target image\n",
    "target_img = load_byte(lizard_byte, TARGET_SIZE_EMOJI) > 0.26\n",
    "\n",
    "target_img = (\n",
    "    target_img[..., 2] | target_img[..., 1] | target_img[..., 0]\n",
    ").astype(jnp.float32)\n",
    "\n",
    "\n",
    "plot_show(zoom(target_img[..., None], 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Creaci칩n del Grafo k=8 (Ubicaci칩n: Antes de la Celda 34)\n",
    "\n",
    "N_nodes_lizard = TARGET_SIZE_EMOJI * TARGET_SIZE_EMOJI # 20*20 = 400\n",
    "MAX_NEIGHBORS = 9\n",
    "CENTER_INDEX = 4\n",
    "\n",
    "# Crear el grafo 8-regular\n",
    "# La funci칩n random_regular_graph requiere que N*d sea par. 400*8 es 3200 (par).\n",
    "G_lizard_k8 = nx.random_regular_graph(d=8, n=N_nodes_lizard) \n",
    "\n",
    "# Generar el tensor de JAX\n",
    "adjacency_indices_padded_lizard = prepare_adjacency_tensor(G_lizard_k8, N_nodes_lizard, MAX_NEIGHBORS, CENTER_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAz4IaTmpxMH"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'perceive': {}, 'update': {}}\n",
    "hyperparams['seed'] = 23\n",
    "hyperparams['lr'] = 0.06\n",
    "hyperparams['batch_size'] = 1\n",
    "hyperparams['num_epochs'] = 3500\n",
    "hyperparams['num_steps'] = 12\n",
    "hyperparams['channels'] = 128\n",
    "hyperparams['periodic'] = 1\n",
    "hyperparams['perceive']['n_kernels'] = 4\n",
    "hyperparams['perceive']['layers'] = [9, 8, 4, 2, 1]\n",
    "hyperparams['perceive']['connections'] = [\n",
    "    'unique', \n",
    "    'unique',\n",
    "    'unique',\n",
    "    'unique',\n",
    "]\n",
    "#Modificamos first kernel por unique\n",
    "\n",
    "init = (\n",
    "    hyperparams['perceive']['n_kernels']\n",
    "    * hyperparams['channels']\n",
    "    * hyperparams['perceive']['layers'][-1]\n",
    "    + hyperparams['channels']\n",
    ")\n",
    "hyperparams['update']['layers'] = (\n",
    "    [init] + [512] * 8 + [256, hyperparams['channels']]\n",
    ")\n",
    "hyperparams['update']['connections'] = ['unique'] * len(\n",
    "    hyperparams['update']['layers']\n",
    ")\n",
    "hyperparams['async_training'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAs7wGiRp0um"
   },
   "outputs": [],
   "source": [
    "TrainState = namedtuple('TrainState', 'param opt_state key')\n",
    "\n",
    "# Create optimizer\n",
    "opt = optax.chain(\n",
    "    optax.clip(100.0),  # Clips by value# @title Funci칩n de Python: prepare_adjacency_tensor para Grafo k=8\n",
    "    optax.adamw(\n",
    "        learning_rate=hyperparams['lr'], b1=0.9, b2=0.99, weight_decay=1e-2\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agregamos la funcion def loss para el experimento Lizard\n",
    "def loss_f(\n",
    "    params, wires, train_x, train_y, periodic, num_steps, async_training, key, \n",
    "    adjacency_indices_padded # <--- ARGUMENTO REQUERIDO\n",
    "):\n",
    "  def eval(params, training):\n",
    "    # Nota: Aqu칤 se llama a v_run_iter_nca y se le pasa el nuevo argumento.\n",
    "    y = v_run_iter_nca(\n",
    "        train_x, \n",
    "        params, \n",
    "        wires, \n",
    "        training, \n",
    "        periodic, \n",
    "        num_steps, \n",
    "        async_training, \n",
    "        key,\n",
    "        adjacency_indices_padded # <--- PASAR AL ITERADOR\n",
    "    )\n",
    "    # La l칩gica de p칠rdida del Lizard compara la salida con el primer canal del objetivo\n",
    "    return jax.numpy.square(y[..., 0] - train_y[..., 0]).sum() \n",
    "\n",
    "  return eval(params, 1), {'hard': eval(params, 0)}\n",
    "\n",
    "\n",
    "val_and_grad = jax.value_and_grad(loss_f, argnums=0, has_aux=True)\n",
    "\n",
    "upd_f = lambda p: p\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(4, 5, 6))\n",
    "def train_step(\n",
    "    train_state, train_x, train_y, wires, periodic, num_steps, async_training, adjacency_indices_padded\n",
    "):\n",
    "  params, opt_state, key = train_state\n",
    "  key, k1 = jax.random.split(key, 2)\n",
    "  (loss, hard), dx = val_and_grad(  \n",
    "      params, wires, train_x, train_y, periodic, num_steps, async_training, k1, adjacency_indices_padded\n",
    "  )\n",
    "  dx, opt_state = opt.update(dx, opt_state, params)\n",
    "  new_params = optax.apply_updates(params, dx)\n",
    "  new_params = upd_f(new_params)\n",
    "  return TrainState(new_params, opt_state, key), loss, hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state, wires = init_state(hyperparams, opt, hyperparams['seed'])\n",
    "key = random.PRNGKey(hyperparams['seed'])\n",
    "\n",
    "loss_soft = []\n",
    "loss_hard = []\n",
    "\n",
    "train_y = jnp.zeros(\n",
    "    shape=(\n",
    "        hyperparams['batch_size'],\n",
    "        TARGET_SIZE_EMOJI,\n",
    "        TARGET_SIZE_EMOJI,\n",
    "        hyperparams['channels'],\n",
    "    )\n",
    ")\n",
    "train_y = train_y.at[:, :, :, 0].set(target_img)\n",
    "\n",
    "train_x = jnp.zeros(\n",
    "    shape=(\n",
    "        hyperparams['batch_size'],\n",
    "        TARGET_SIZE_EMOJI,\n",
    "        TARGET_SIZE_EMOJI,\n",
    "        hyperparams['channels'],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Seed\n",
    "train_x = train_x.at[:, TARGET_SIZE_EMOJI // 2, TARGET_SIZE_EMOJI // 2, :].set(\n",
    "    1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpKQVDKvp2bk"
   },
   "outputs": [],
   "source": [
    "for i in range(hyperparams['num_epochs']):\n",
    "  train_state, soft_loss, hard_loss = train_step(\n",
    "      train_state,\n",
    "      train_x,\n",
    "      train_y,\n",
    "      wires,\n",
    "      hyperparams['periodic'],\n",
    "      hyperparams['num_steps'],\n",
    "      hyperparams['async_training'],\n",
    "      adjacency_indices_padded_lizard,\n",
    "  )\n",
    "  loss_soft.append(soft_loss)\n",
    "  loss_hard.append(hard_loss['hard'])\n",
    "\n",
    "  if i % 100 == 0:\n",
    "    clear_output(wait=True)\n",
    "    plot_training_progress(loss_soft, loss_hard, 1)\n",
    "    print(i, soft_loss, hard_loss['hard'])\n",
    "\n",
    "\n",
    "clear_output(wait=True)\n",
    "plot_training_progress(loss_soft, loss_hard, 1, 'lizard_logo_loss.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0hAuSO7p59M"
   },
   "outputs": [],
   "source": [
    "\"\"\"Growing the Lizard\n",
    "\n",
    "Visualization of the recurrent dynamics of the learned circuit.\n",
    "\"\"\"\n",
    "\n",
    "grid = train_x[0]\n",
    "\n",
    "params, opt_state, key = train_state\n",
    "wires = wires\n",
    "training = False\n",
    "periodic = True\n",
    "\n",
    "print(grid.shape)\n",
    "frames = []\n",
    "nca_state = grid\n",
    "frames.append(zoom(nca_state[:, :, 0], 8))\n",
    "for i in range(12):\n",
    "  nca_state = run_sync(nca_state, params, wires, training, periodic, adjacency_indices_padded_lizard)\n",
    "  frames.append(zoom(nca_state[:, :, 0], 8))\n",
    "\n",
    "visualize(frames, 'lizard.gif', 5)\n",
    "Image('lizard.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyN0n7cxM20B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhyhcTixqC9O"
   },
   "outputs": [],
   "source": [
    "\"\"\"Growing the Lizard: visualization of the hidden states\n",
    "\n",
    "Visualization of the first 32 channels\n",
    "\"\"\"\n",
    "\n",
    "params, opt_state, key = train_state\n",
    "wires = wires\n",
    "training = False\n",
    "periodic = True\n",
    "\n",
    "frames = []\n",
    "\n",
    "nca_state = train_x[0]\n",
    "# Create subplot grid for all 32 channels\n",
    "initial_grid = np.vstack([\n",
    "    np.hstack([zoom(nca_state[:, :, 8 * j + i], 4) for i in range(8)])\n",
    "    for j in range(4)\n",
    "])\n",
    "frames.append(initial_grid)\n",
    "\n",
    "for i in range(12):\n",
    "  nca_state = run_sync(nca_state, params, wires, training, periodic, adjacency_indices_padded_lizard)\n",
    "  channel_grid = np.vstack([\n",
    "      np.hstack([zoom(nca_state[:, :, 8 * j + i], 4) for i in range(8)])\n",
    "      for j in range(4)\n",
    "  ])\n",
    "  frames.append(channel_grid)\n",
    "\n",
    "visualize(frames, 'hidden_lizard.gif', 5)\n",
    "Image('hidden_lizard.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda de visualizacion Grafo\n",
    "\n",
    "# Usamos G_lizard_k8, creado en la Celda ~34.\n",
    "# N_total_nodes es 400 para el Lizard (20*20).\n",
    "\n",
    "# 1. Fijar las posiciones de los nodos usando un algoritmo de NetworkX (ej. spring_layout)\n",
    "# Se recomienda usar el mismo 'seed' para la reproducibilidad del layout:\n",
    "pos = nx.spring_layout(G_lizard_k8, seed=42) \n",
    "\n",
    "# 2. Simular para obtener los datos de la animaci칩n\n",
    "# DEBES obtener la secuencia de estados del CA (nodos en formato 1D: N_nodos x C)\n",
    "# Esto requiere una funci칩n de simulaci칩n que devuelva una lista de estados.\n",
    "# Por ahora, solo simula y guarda los estados.\n",
    "\n",
    "# NOTA: Debes ejecutar la simulaci칩n y guardar los estados \n",
    "# (ej. solo el Canal 0) en una lista llamada `states_history`.\n",
    "# Puedes reutilizar la l칩gica de run_sync, pero guardando todos los pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def update_animation(frame_num, G, pos, states_history, ax):\n",
    "    ax.clear()\n",
    "\n",
    "    # Obtener el estado del CA para este frame (ej. solo el Canal 0, el estado visible)\n",
    "    current_state = states_history[frame_num][..., 0] \n",
    "\n",
    "    # Mapear el estado del nodo a un color\n",
    "    # Usaremos el estado del nodo para el color (ej. 1.0 = Rojo, 0.0 = Azul)\n",
    "    node_colors = plt.cm.RdYlBu(current_state) \n",
    "\n",
    "    # 1. Dibujar las aristas del grafo (estructura fija)\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, alpha=0.5, edge_color='gray')\n",
    "\n",
    "    # 2. Dibujar los nodos con el color mapeado al estado actual\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, \n",
    "        pos, \n",
    "        node_color=node_colors, \n",
    "        node_size=20, # Tama침o fijo del nodo\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"Paso de Evoluci칩n: {frame_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumimos que 'states_history' contiene N_steps arrays de shape (N_nodes, C)\n",
    "N_steps = len(states_history) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, \n",
    "    update_animation, \n",
    "    frames=N_steps, \n",
    "    fargs=(G_lizard_k8, pos, states_history, ax), # Argumentos para update_animation\n",
    "    interval=200, # 5 frames por segundo (1000ms / 200)\n",
    "    blit=False, \n",
    "    repeat=True\n",
    ")\n",
    "\n",
    "# Mostrar o guardar la animaci칩n\n",
    "plt.show() # Para visualizar en el notebook (requiere ambiente interactivo)\n",
    "\n",
    "# Para guardar en MP4 (puede requerir ffmpeg):\n",
    "# ani.save('animated_lizard_graph.mp4', writer='ffmpeg', fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxV5Tb_5NETv"
   },
   "outputs": [],
   "source": [
    "\"\"\"Analysis of Gate Distributions:\n",
    "\n",
    "This section investigates the learned gate distributions\n",
    "for the Update and Perceive Networks in the growing Lizard model.\n",
    "\"\"\"\n",
    "\n",
    "params, opt_state, key = train_state\n",
    "analyze_circuit = jax.tree_util.tree_map(\n",
    "    lambda x: jax.numpy.argmax(x, axis=-1), params\n",
    ")\n",
    "count_gates = jax.tree_util.tree_map(\n",
    "    lambda x: ((x != 3) & (x != 5)).sum(), analyze_circuit\n",
    ")\n",
    "total_gates = sum(x for x in jax.tree_util.tree_leaves(count_gates))\n",
    "print('TOTAL GATES:  ', total_gates)\n",
    "\n",
    "perceive_networks = analyze_circuit['perceive']\n",
    "update_networks = analyze_circuit['update']\n",
    "\n",
    "# Perceive Network\n",
    "perceive_networks_stats = [\n",
    "    x.reshape(-1) for x in jax.tree_util.tree_leaves(perceive_networks)\n",
    "]\n",
    "perceive_net = jax.numpy.concatenate(perceive_networks_stats, axis=0)\n",
    "perceive_net = perceive_net[((perceive_net != 3) & (perceive_net != 5))]\n",
    "\n",
    "\n",
    "plot_hist_gates(\n",
    "    perceive_net,\n",
    "    'lizard_perceive_gates.svg',\n",
    "    'Distribution of Gates (Perceive Network)',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQvbZ2uTNG1d"
   },
   "outputs": [],
   "source": [
    "# Update Network\n",
    "update_networks_stats = [\n",
    "    x.reshape(-1) for x in jax.tree_util.tree_leaves(update_networks)\n",
    "]\n",
    "update_net = jax.numpy.concatenate(update_networks_stats, axis=0)\n",
    "update_net = update_net[((update_net != 3) & (update_net != 5))]\n",
    "\n",
    "plot_hist_gates(\n",
    "    update_net,\n",
    "    'lizard_update_gates.svg',\n",
    "    'Distribution of Gates (Update Network)',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "https://github.com/google-research/self-organising-systems/blob/master/notebooks/diffLogic_CA.ipynb",
     "timestamp": 1761326495385
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
